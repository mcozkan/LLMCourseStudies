{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956c7a28-3156-4819-a1a2-7e44105fb29e",
   "metadata": {
    "id": "956c7a28-3156-4819-a1a2-7e44105fb29e"
   },
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='Image-to-Text-LLM-700x450.png'))  # .png, .jpeg de olabilir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kKdPwjHIAfGc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "error",
     "timestamp": 1751592453221,
     "user": {
      "displayName": "Murat Cagri Ã–zkan",
      "userId": "05575202815760941283"
     },
     "user_tz": -180
    },
    "id": "kKdPwjHIAfGc",
    "outputId": "5627ee34-7497-4172-9df4-4091831bed87"
   },
   "outputs": [],
   "source": [
    "![Project Overview](assets/Image-to-Text-LLM-700x450.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9fc48-731b-4b87-b09e-3fcf0ea9466c",
   "metadata": {
    "id": "5dd9fc48-731b-4b87-b09e-3fcf0ea9466c"
   },
   "source": [
    "### **1. Loading Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9fbaef-c8af-4e3e-8db4-ad08a350d84e",
   "metadata": {
    "id": "bd9fbaef-c8af-4e3e-8db4-ad08a350d84e"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import BaseTool, Tool\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd9909-e224-4594-beb6-b6d5112776a8",
   "metadata": {
    "id": "1afd9909-e224-4594-beb6-b6d5112776a8"
   },
   "source": [
    "### **2. Load Environment - Do not forget! If you passed this step you can not use your API key!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d99ea2-b460-4721-acdb-20c786d9f3cc",
   "metadata": {
    "id": "61d99ea2-b460-4721-acdb-20c786d9f3cc"
   },
   "source": [
    "#### To use the OpenAI API, you need to obtain an API key and store it in a `.env` file as follows:\n",
    "\n",
    "##### **OPENAI_API_KEY=sk-...**\n",
    "\n",
    "Then you may run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8980a4-2d60-4975-9e12-0985cf53fe91",
   "metadata": {
    "id": "ab8980a4-2d60-4975-9e12-0985cf53fe91",
    "outputId": "dccbee5b-6a64-42c6-c451-850c005c7b5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976a68a-5c2e-457f-bd16-e848535ccec2",
   "metadata": {
    "id": "5976a68a-5c2e-457f-bd16-e848535ccec2"
   },
   "source": [
    "### **3. Agent Tool Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d94bfe-a7c0-4c30-a176-10d6ef46bc01",
   "metadata": {
    "id": "70d94bfe-a7c0-4c30-a176-10d6ef46bc01"
   },
   "source": [
    "**Pretrained Model Link :** https://huggingface.co/Salesforce/blip-image-captioning-base<br>\n",
    "**Paper :** https://arxiv.org/abs/2201.12086\n",
    "\n",
    "**BLIP (Bootstrapped Language-Image Pretraining):** is a powerful and modern vision-language model that brings together images and texts to generate better captions, answer visual questions (VQA), and improve image-level understanding.\n",
    "\n",
    "**Aim:** To analyze an image and generate text in natural language (e.g., \"A red mug on the table.\")\n",
    "\n",
    "**Area of use:** Automatic product descriptions, social media captions, visual descriptions for people with visual impairments, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876fcea-93c9-4624-b630-8557ebe70d36",
   "metadata": {
    "id": "c876fcea-93c9-4624-b630-8557ebe70d36"
   },
   "outputs": [],
   "source": [
    "def generate_caption(image_path: str):\n",
    "    # Downloads model and processor from the HuggingFace and save it to hte Cache...\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    # Captioning is processing on this step...\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    # Color code - RGB- is configured...\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    #The processor converts the input image into a tensor format required by the model.  (pt :PyTorch, tf : TensorFlow)\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    # Processes the image and returns token IDs in PyTorch tensor format\n",
    "    output = model.generate(**inputs)\n",
    "    # Converts the model output IDs into a natural language caption\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "    return caption # # At the end of this process, the generated text will be sent to the LLM Agent for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0bb98-7a79-4bc4-b123-c4030cf3f7b1",
   "metadata": {
    "id": "aeb0bb98-7a79-4bc4-b123-c4030cf3f7b1"
   },
   "source": [
    "### **4. Agent Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f335e60-6452-492d-bb1d-ece52f2d5ba2",
   "metadata": {
    "id": "5f335e60-6452-492d-bb1d-ece52f2d5ba2"
   },
   "source": [
    "##### a. Model definition using  LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb8553-601e-43d9-9d85-dc411504d658",
   "metadata": {
    "id": "5dcb8553-601e-43d9-9d85-dc411504d658"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt4\", temperature = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9e969-1ba9-41b7-831e-c7c69843afc9",
   "metadata": {
    "id": "0aa9e969-1ba9-41b7-831e-c7c69843afc9"
   },
   "source": [
    "##### b. I used here Langchain tool object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881802f-f04d-4b5b-8d79-df0809c7c5c7",
   "metadata": {
    "id": "3881802f-f04d-4b5b-8d79-df0809c7c5c7"
   },
   "outputs": [],
   "source": [
    "caption_tool = Tool (name = \"image_caption_tool\", function = generate_caption,\n",
    "                     description = \"Gets the image path, and make a sentence that explains what the image is.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04505f71-ad69-4b56-a959-e24c7836726f",
   "metadata": {
    "id": "04505f71-ad69-4b56-a959-e24c7836726f"
   },
   "source": [
    "##### c. InÄ±tializing Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74bf7d-b66c-498b-9af6-15012c75d1d3",
   "metadata": {
    "id": "ae74bf7d-b66c-498b-9af6-15012c75d1d3"
   },
   "outputs": [],
   "source": [
    "agent = Initialize_agent(\n",
    "    tools = [caption_tool],\n",
    "    llm = llm,\n",
    "    # If you would like to create more complex structures, it will be better to use other Agent Types.\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b88324-5000-4fbd-a3cb-09123a9ae92c",
   "metadata": {
    "id": "c5b88324-5000-4fbd-a3cb-09123a9ae92c"
   },
   "source": [
    "### **5. Result And Streamlit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ebd881-28eb-4289-a7fe-11d4faa9571f",
   "metadata": {
    "id": "02ebd881-28eb-4289-a7fe-11d4faa9571f"
   },
   "outputs": [],
   "source": [
    "st.title(\"ðŸ¤–ðŸ’¬ Creates a product comment and generates SEO-compliant tags using the loaded image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaebf17-7e31-48c9-9822-8145a8d25439",
   "metadata": {
    "id": "feaebf17-7e31-48c9-9822-8145a8d25439"
   },
   "outputs": [],
   "source": [
    "image = st.file_uploader(\"Please Load a Product Image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "if image:\n",
    "    # This code creates a file named 'temp.jpg', which will be used by the application and the model each time a new image is uploaded.\n",
    "    with open(\"temp.jpg\", \"wb\") as f:\n",
    "        f.write(image.read())\n",
    "\n",
    "    st.image(\"temp.jpg\", caption=\"The Image has been loaded\", use_container_width=True)\n",
    "    st.info(\"Analyzing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2107f-a0c2-40b6-96c7-b1e80a2deed1",
   "metadata": {
    "id": "75a2107f-a0c2-40b6-96c7-b1e80a2deed1"
   },
   "outputs": [],
   "source": [
    "# I used a Chain of Thought (CoT) prompt to guide the model through step-by-step reasoning.\n",
    "prompt_CoT = \"\"\"\n",
    "    Image file path: temp.jpg\n",
    "\n",
    "    1. Use the image_caption_tool directly to analyze the content of the given image.\n",
    "       This tool returns a sentence that describes what is shown in the image.\n",
    "       Do not guess without using the tool.\n",
    "\n",
    "    2. Based on the generated description:\n",
    "        - Create a short and creative product title.\n",
    "        - Write a unique product description in 3â€“4 sentences.\n",
    "        - Suggest 10 SEO-friendly tags that define the product.\n",
    "\n",
    "    Format your response clearly, with properly aligned step numbers one by one and structured output.\n",
    "    Use the toolâ€™s output directly and indicate it.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ccdef-2758-4cbb-b0a9-01628abc0503",
   "metadata": {
    "id": "da1ccdef-2758-4cbb-b0a9-01628abc0503"
   },
   "outputs": [],
   "source": [
    "# I added a try-except block, although no error has occurred so far.\n",
    "# This ensures that if any error does happen, Streamlit won't crash and the error message will be shown to the user.\n",
    "    try:\n",
    "        output = agent.invoke(prompt_CoT)\n",
    "        st.success(\"âœ… Analyze Over and Comment - Tags Generated\")\n",
    "        st.write(output[\"output\"])\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error!{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_fg4N3EL-Dsi",
   "metadata": {
    "id": "_fg4N3EL-Dsi"
   },
   "source": [
    "##### To run this project please go to terminal and write the command below and enter:<br>\n",
    "    streamlit run /project_Path/project_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yHLUER7Z-NPC",
   "metadata": {
    "id": "yHLUER7Z-NPC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
